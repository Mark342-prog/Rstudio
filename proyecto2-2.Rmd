---
title: "proyecto2"
output:
  pdf_document: default
  word_document: default
  html_document: default
date: "2025-01-30"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

path <- "C:/Users/markp/OneDrive/Documentos/house/train.csvv"

if (file.exists(path)) {
  content <- read.csv(path)
  print(content)
} else {
  print("El archivo no existe. Verifica la ruta.")
}
library(cluster) 
library(e1071)
library(mclust) 
library(fpc) 
library(NbClust) 
library(factoextra)
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(caret)
library(car)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
library(randomForest)
library(mlr)
path <- "C:/Users/markp/OneDrive/Documentos/house/train.csv"
datos <- read_csv(path, locale = locale(encoding = "UTF-8"), show_col_types = FALSE)
knitr::opts_chunk$set(echo = TRUE)


```
## Utilizando los datos
Vamos a volver a utilizar los datos de train una vez más, esta vez utilizaremos todos


```{r variables, echo=FALSE}
# Cargar paquetes necesarios
library(dplyr)
datos <- datos %>% select(-Id)
datos[is.na(datos$LotFrontage), "LotFrontage"] <- median(datos$LotFrontage, na.rm = TRUE)
datos[is.na(datos$MasVnrArea), "MasVnrArea"] <- median(datos$MasVnrArea, na.rm = TRUE)
datos[is.na(datos$BsmtFinSF1), "BsmtFinSF1"] <- median(datos$BsmtFinSF1, na.rm = TRUE)
datos[is.na(datos$BsmtFinSF2), "BsmtFinSF2"] <- median(datos$BsmtFinSF2, na.rm = TRUE)
datos[is.na(datos$BsmtUnfSF), "BsmtUnfSF"] <- median(datos$BsmtUnfSF, na.rm = TRUE)
datos[is.na(datos$TotalBsmtSF), "TotalBsmtSF"] <- median(datos$TotalBsmtSF, na.rm = TRUE)
datos[is.na(datos$GarageYrBlt), "GarageYrBlt"] <- median(datos$GarageYrBlt, na.rm = TRUE)
datos[is.na(datos$GarageCars), "GarageCars"] <- median(datos$GarageCars, na.rm = TRUE)
datos[is.na(datos$GarageArea), "GarageArea"] <- median(datos$GarageArea, na.rm = TRUE)
datos[is.na(datos$MiscVal), "MiscVal"] <- median(datos$MiscVal, na.rm = TRUE)

# Eliminar columna innecesaria (puedes modificar según sea necesario)
datos$Alley <- NULL  # Ejemplo: eliminamos Alley si tiene demasiados NA

# Seleccionar solo columnas numéricas
colNum <- c("LotFrontage", "LotArea", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", 
            "BsmtUnfSF", "TotalBsmtSF", "1stFlrSF", "2ndFlrSF", "GrLivArea", 
            "GarageYrBlt", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", 
            "EnclosedPorch", "ScreenPorch", "PoolArea", "MiscVal", "SalePrice")

numericas <- datos[, colNum]


datos <- datos %>% mutate_if(is.character, as.factor)


summary(datos)


```
## haciendo el Arból
A continuacuón vamos a intentar hacer un árbol y intentando hacerlo de manera en que muestre patrones que previamente puede que fuesen omitidas
```{r tree, echo=FALSE}

datos <- datos %>% mutate_if(is.character, as.factor)


arbol <- rpart(SalePrice ~ ., data = datos, method = "anova")


rpart.plot(arbol)

```


## Resultados

La variable más importante es OverallQual.

Si OverallQual < 8, el precio promedio baja a 158,000 USD.
Si OverallQual ≥ 8, el precio promedio sube a 305,000 USD.
Para viviendas con OverallQual < 8 (84% de los datos):

El barrio (Neighborhood) influye bastante. Si pertenece a ciertos barrios de menor precio, el valor disminuye.

1stFlrSF < 1051 pies cuadrados indica precios bajos (~118,000 USD).

GrLivArea (área habitable sobre el suelo) también es clave. Si es menor a 1,120, el precio baja (~135,000 USD), pero si es mayor, sube (~185,000 USD).

BsmtFinSF1 (área terminada del sótano) también juega un rol en el precio.
Para viviendas con OverallQual ≥ 8 (16% de los datos):

Si OverallQual = 8, entonces GrLivArea (< 1972 vs. ≥ 1972) sigue siendo una variable clave. Casas con más área tienden a costar más (~315,000 USD).

Si OverallQual ≥ 9, la casa es más cara (~388,000 a 450,000 USD), y nuevamente el tamaño (GrLivArea) es crucial.
## Cambiando profundidades
Vamos a ir cambiando la profundidad del gráfico entre más vamos 
vamos a empezar con depth 3
```{r tree2, echo=FALSE}

arbol2 <- rpart(SalePrice ~ ., data = datos, control = rpart.control(maxdepth = 3))


rpart.plot(arbol2)
```

Ahora sigamos con deapth 4

```{r tree3, echo=FALSE}

arbol3 <- rpart(SalePrice ~ ., data = datos, control = rpart.control(maxdepth = 4))


rpart.plot(arbol3)
```


Ahora sigamos con deapth 5

```{r tree4, echo=FALSE}

arbol4 <- rpart(SalePrice ~ ., data = datos, control = rpart.control(maxdepth = 5))


rpart.plot(arbol4)
```

Lamentablemente no cambia mucho o nada, pero podemos interpretar cosas con lo que hemos aprendido

## Analizando los árboles

La calidad general (OverallQual) es el factor más importante

El nodo raíz divide los datos en dos ramas principales basadas en si OverallQual < 8 o no.
Esto indica que la calidad general de la vivienda es un predictor clave del precio de venta.
Dentro de casas con menor calidad (OverallQual < 8), el vecindario juega un rol importante

Si la casa está en ciertos vecindarios (Blueste, BrDale, etc.), el precio tiende a ser más bajo.
Para estos vecindarios, el tamaño de la primera planta (1stFlrSF) afecta el precio.
En casas con mejor calidad (OverallQual ≥ 8), el tamaño de la vivienda es clave

La variable GrLivArea (área habitable sobre el suelo) es un factor determinante en el precio.
Si GrLivArea es mayor a 2229, los precios son significativamente más altos.
Los precios más altos ocurren en casas con alta calidad y gran área habitable

Las casas con OverallQual ≥ 9 y GrLivArea ≥ 2229 tienen los precios más elevados .

## Creando varaibles

Vamos a crear variales para que se pueda analizar mejor las cosas
```{r variable, echo=FALSE}
media_price <- mean(datos$SalePrice, na.rm = TRUE)
sd_price <- sd(datos$SalePrice, na.rm = TRUE)

limite_bajo <- media_price - 0.5 * sd_price
limite_alto <- media_price + 0.5 * sd_price

datos <- datos %>%
  mutate(CategoriaPrecio = case_when(
    SalePrice < limite_bajo ~ "Económica",
    SalePrice >= limite_bajo & SalePrice <= limite_alto ~ "Intermedia",
    SalePrice > limite_alto ~ "Cara"
  ))

datos$CategoriaPrecio <- factor(datos$CategoriaPrecio, levels = c("Económica", "Intermedia", "Cara"))


table(datos$CategoriaPrecio)


```

Ok, ya tenemos los datos, ahora apliquemoslos a ver que sucede



```{r statistics2, echo=FALSE}

datos$Categoria <- cut(datos$SalePrice,
                        breaks = c(-Inf, 141000, 220000, Inf),
                        labels = c("Económica", "Intermedia", "Cara"))

datos$Categoria <- as.factor(datos$Categoria)


carros_train <- subset(datos, select = -SalePrice)


arbol_clasificacion <- rpart(Categoria ~ ., data = carros_train, method = "class")

rpart.plot(arbol_clasificacion, type = 4, extra = 104, box.palette = "RdYlGn", shadow.col = "gray")
```



Variables como 1stFt SF (tamaño del primer piso), GarageArea (área del garaje), GrLivArea (área habitable) y TotalBsmtSF (área del sótano) determinan la categoría asignada.

Ejemplo: Si 1stFt SF < 1051, el 50% se clasifica como "Económica".


Los porcentajes indican la proporción de propiedades que cumplen ciertas condiciones y caen en una categoría.

Ejemplo: Bajo GarageArea < 301, el 21% se clasifica como "Intermedia".

Con esto en mente logramos apreciar como se logran apreciar las nuevas subcategorías creadas gracias a la nueva variable

## Haciendo un Random Forest


```{r forest, echo=FALSE}
library(caret)
library(dplyr)

datos_clean <- na.omit(datos)  
datos_clean <- datos_clean %>% mutate_if(is.character, as.factor) 

# ------------------------------------------------------------------------
# Paso 3: Dividir en entrenamiento y prueba
set.seed(123)


trainData <- datos[trainIndex, ]
testData <- datos[-trainIndex, ]

# Ajustar los niveles
testData <- ajustar_niveles(trainData, testData)

# Eliminar filas con NA en testData luego del ajuste de niveles.
testData <- testData[complete.cases(testData),]

# Asegurar que la columna categoria sea factor en los dos dataframes.
trainData$Categoria <- as.factor(trainData$Categoria)
testData$Categoria <- as.factor(testData$Categoria)

# Eliminar filas con NA en Categoria
trainData <- trainData[!is.na(trainData$Categoria), ]

# Eliminar filas con NA restantes
trainData <- trainData[complete.cases(trainData), ]

# Modelo Random Forest
modelo_rf <- randomForest(
  Categoria ~ .,
  data = trainData %>% select(-SalePrice),
  ntree = 500, # Número de árboles (ajustable)
  mtry = floor(sqrt(ncol(trainData %>% select(-SalePrice)))), # Número de variables consideradas en cada división
  importance = TRUE # Para calcular la importancia de las variables
)

# Realizar predicciones
predicciones_rf <- predict(modelo_rf, newdata = testData %>% select(-SalePrice))

# Asegurarse de que predicciones sea un factor y que los niveles coincidan.
predicciones_rf <- factor(predicciones_rf, levels = levels(testData$Categoria))

# Matriz de confusión
conf_matrix_rf <- confusionMatrix(predicciones_rf, testData$Categoria)
print(conf_matrix_rf)

# Importancia de las variables
importance(modelo_rf)

```

Esto significa que tu modelo clasifica correctamente el 84.11% de las observaciones en el conjunto de prueba. Es una buena precisión, lo que sugiere que el modelo tiene un rendimiento sólido.

95% CI (Intervalo de Confianza del 95%): (0.7728, 0.8954)
Esto indica que, con un 95% de confianza, la precisión real del modelo se encuentra entre el 77.28% y el 89.54%.

No Information Rate (Tasa de No Información): 0.4901
Esta es la precisión que obtendrías si siempre predijeras la clase más frecuente. En este caso, es del 49.01%.

P-Value [Acc > NIR]: < 2e-16
Este valor P es extremadamente bajo, lo que significa que la precisión de tu modelo es significativamente mejor que la tasa de no información. Esto indica que el modelo está aprendiendo patrones reales en los datos
.
Kappa: 0.73

Mcnemar's Test P-Value: 0.2102

## Resumiendo lo aprendido

A lo largo de este proceso logramos deducir muchas cosas, entre una de ellas es que una de las mayores Si que existen coorelaciones no vistas anteriormente, por ejemplo en un árbol se logro apreciar como los pisos, areas de vivienda y areas extras, lograrón hacer que muchas casas fueran categorizadas de maneras muy distintas, al mismo tiempo logramos precenciar como cada árbol evoluciono, Se logró predecir con una incertidumbre bastante moderada, aunque algo alta, cada uno logro hacer una predicción buena y creo que fue un modelado exitoso